{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic data generation\n",
    "\n",
    "This notebook fabricates a miniature dataset that emulates the signals Monumental Labs could capture from its 7-axis KUKA carving cells. The goal is to create three related tables: job-level summaries, CAM toolpath parameters, and high-frequency telemetry sampled during each cut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(42)\n",
    "rng = np.random.default_rng(42)\n",
    "OUTPUT_DIR = Path(\"../data\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "materials = pd.DataFrame(\n",
    "    [\n",
    "        {\"material\": \"limestone\", \"base_finish\": 150, \"price_per_cm3\": 0.16, \"complexity_factor\": 120},\n",
    "        {\"material\": \"marble\", \"base_finish\": 210, \"price_per_cm3\": 0.25, \"complexity_factor\": 150},\n",
    "        {\"material\": \"granite\", \"base_finish\": 260, \"price_per_cm3\": 0.32, \"complexity_factor\": 180},\n",
    "    ]\n",
    ")\n",
    "\n",
    "def synthesize_jobs(n_jobs: int = 16) -> pd.DataFrame:\n",
    "    start_date = datetime(2025, 10, 1, 8)\n",
    "    choices = materials.sample(n=n_jobs, replace=True, random_state=7).reset_index(drop=True)\n",
    "    complexity = np.round(rng.uniform(0.25, 0.85, size=n_jobs), 2)\n",
    "    base_volume = rng.normal(30000, 6000, size=n_jobs)\n",
    "    volume_removed = np.clip(base_volume * (0.6 + complexity), 12000, 52000)\n",
    "\n",
    "    finish_minutes = np.round(\n",
    "        choices[\"base_finish\"].values\n",
    "        + complexity * choices[\"complexity_factor\"].values\n",
    "        + rng.normal(0, 18, size=n_jobs),\n",
    "        1,\n",
    "    )\n",
    "    quoted = np.round(volume_removed * choices[\"price_per_cm3\"].values * (0.8 + complexity), 2)\n",
    "    scheduled = [start_date + timedelta(hours=i * 6 + rng.uniform(-1, 1)) for i in range(n_jobs)]\n",
    "\n",
    "    jobs = pd.DataFrame(\n",
    "        {\n",
    "            \"job_id\": [f\"J{i+1:03d}\" for i in range(n_jobs)],\n",
    "            \"material\": choices[\"material\"],\n",
    "            \"geometry_complexity\": complexity,\n",
    "            \"volume_removed_cm3\": volume_removed.astype(int),\n",
    "            \"finish_minutes\": finish_minutes,\n",
    "            \"quoted_price_usd\": quoted,\n",
    "            \"scheduled_start\": [dt.isoformat() for dt in scheduled],\n",
    "        }\n",
    "    )\n",
    "    return jobs\n",
    "\n",
    "\n",
    "def synthesize_toolpaths(jobs: pd.DataFrame) -> pd.DataFrame:\n",
    "    records: List[dict] = []\n",
    "    tool_ids = np.array([\"Tool_A\", \"Tool_B\", \"Tool_C\", \"Tool_Rough\", \"Tool_Finish\"])\n",
    "\n",
    "    for idx, job in jobs.iterrows():\n",
    "        n_paths = int(rng.integers(2, 4))\n",
    "        shares = rng.uniform(0.18, 0.45, size=n_paths)\n",
    "        shares /= shares.sum()\n",
    "        for seq, share in enumerate(shares):\n",
    "            complexity = job[\"geometry_complexity\"]\n",
    "            records.append(\n",
    "                {\n",
    "                    \"toolpath_id\": f\"T{idx * 4 + seq:03d}\",\n",
    "                    \"job_id\": job[\"job_id\"],\n",
    "                    \"feed_mm_min\": max(int(rng.normal(1100 - 180 * complexity, 110)), 400),\n",
    "                    \"rpm\": max(int(rng.normal(5200 + 1200 * (1 - complexity), 380)), 3200),\n",
    "                    \"spindle_current_a\": round(rng.normal(15 + 6 * complexity, 1.4), 2),\n",
    "                    \"contact_time_s\": round(job[\"finish_minutes\"] * 60 * share * rng.uniform(0.95, 1.08), 1),\n",
    "                    \"tool_id\": rng.choice(tool_ids),\n",
    "                }\n",
    "            )\n",
    "\n",
    "    return pd.DataFrame.from_records(records)\n",
    "\n",
    "\n",
    "def synthesize_telemetry(toolpaths: pd.DataFrame, points_per_path: int = 12) -> pd.DataFrame:\n",
    "    base_time = datetime(2025, 10, 30, 12)\n",
    "    telemetry_rows = []\n",
    "    for _, tp in toolpaths.iterrows():\n",
    "        start_offset = rng.uniform(-6, 6)\n",
    "        timestamp = base_time + timedelta(minutes=float(start_offset))\n",
    "        step = tp[\"contact_time_s\"] / points_per_path\n",
    "        for _ in range(points_per_path):\n",
    "            timestamp += timedelta(seconds=float(step))\n",
    "            telemetry_rows.append(\n",
    "                {\n",
    "                    \"timestamp\": timestamp.isoformat(),\n",
    "                    \"toolpath_id\": tp[\"toolpath_id\"],\n",
    "                    \"spindle_current_a\": round(tp[\"spindle_current_a\"] * rng.uniform(0.92, 1.08), 2),\n",
    "                    \"vibration_g\": round(max(rng.normal(0.08 + 0.04 * rng.random(), 0.02), 0.02), 3),\n",
    "                    \"coolant_flow_lpm\": round(max(rng.normal(6.5, 0.5), 5.2), 2),\n",
    "                }\n",
    "            )\n",
    "\n",
    "    return pd.DataFrame(telemetry_rows)\n",
    "\n",
    "\n",
    "jobs_df = synthesize_jobs()\n",
    "toolpaths_df = synthesize_toolpaths(jobs_df)\n",
    "telemetry_df = synthesize_telemetry(toolpaths_df)\n",
    "\n",
    "jobs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toolpaths_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telemetry_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_df.to_csv(OUTPUT_DIR / \"jobs.csv\", index=False)\n",
    "toolpaths_df.to_csv(OUTPUT_DIR / \"toolpaths.csv\", index=False)\n",
    "telemetry_df.to_csv(OUTPUT_DIR / \"telemetry.csv\", index=False)\n",
    "\n",
    "len(jobs_df), len(toolpaths_df), len(telemetry_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have reproducible CSVs ready for DuckDB modeling and dashboarding. If you prefer a scriptable form, run `python scripts/generate_data.py` to emit the same synthetic data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
  "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
